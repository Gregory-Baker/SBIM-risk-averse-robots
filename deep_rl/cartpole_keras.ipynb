{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WARNING:tensorflow:From <ipython-input-4-1fd6d5a7c49f>:62: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greg/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/greg/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:91: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1fd6d5a7c49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1fd6d5a7c49f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1fd6d5a7c49f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1fd6d5a7c49f>\u001b[0m in \u001b[0;36m_train_critic\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    124\u001b[0m \t\t\t\t\t[new_state, target_action])[0][0]\n\u001b[1;32m    125\u001b[0m                                 \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfuture_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m                                           np.expand_dims(sparse_coo.col, 1)), 1)\n\u001b[1;32m   2477\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m         self.name, self.get_shape(), self._dtype.name)\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "solving pendulum using actor-critic model\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "\tdef __init__(self, env, sess):\n",
    "\t\tself.env  = env\n",
    "\t\tself.sess = sess\n",
    "\n",
    "\t\tself.learning_rate = 0.001\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.epsilon_decay = .995\n",
    "\t\tself.gamma = .95\n",
    "\t\tself.tau   = .125\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                               Actor Model                             #\n",
    "\t\t# Chain rule: find the gradient of chaging the actor network params in  #\n",
    "\t\t# getting closest to the final value network predictions, i.e. de/dA    #\n",
    "\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "\t\t# ===================================================================== #\n",
    "\n",
    "\t\tself.memory = deque(maxlen=2000)\n",
    "\t\tself.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "\t\t_, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "\t\tself.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "\t\t\t[None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "\t\t\n",
    "\t\tactor_model_weights = self.actor_model.trainable_weights\n",
    "\t\tself.actor_grads = tf.gradients(self.actor_model.output, \n",
    "\t\t\tactor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "\t\tgrads = zip(self.actor_grads, actor_model_weights)\n",
    "\t\tself.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              Critic Model                             #\n",
    "\t\t# ===================================================================== #\t\t\n",
    "\n",
    "\t\tself.critic_state_input, self.critic_action_input, \\\n",
    "\t\t\tself.critic_model = self.create_critic_model()\n",
    "\t\t_, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "\t\tself.critic_grads = tf.gradients(self.critic_model.output, \n",
    "\t\t\tself.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\t\t\n",
    "\t\t# Initialize for later gradient calculations\n",
    "\t\tself.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Definitions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef create_actor_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.observation_space.shape)\n",
    "\t\th1 = Dense(24, activation='relu')(state_input)\n",
    "\t\th2 = Dense(48, activation='relu')(h1)\n",
    "\t\th3 = Dense(24, activation='relu')(h2)\n",
    "\t\toutput = Dense(self.env.action_space.shape[0], activation='relu')(h3)\n",
    "\t\t\n",
    "\t\tmodel = Model(input=state_input, output=output)\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\tdef create_critic_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.observation_space.shape)\n",
    "\t\tstate_h1 = Dense(24, activation='relu')(state_input)\n",
    "\t\tstate_h2 = Dense(48)(state_h1)\n",
    "\t\t\n",
    "\t\taction_input = Input(shape=self.env.action_space.shape)\n",
    "\t\taction_h1    = Dense(48)(action_input)\n",
    "\t\t\n",
    "\t\tmerged    = Add()([state_h2, action_h1])\n",
    "\t\tmerged_h1 = Dense(24, activation='relu')(merged)\n",
    "\t\toutput = Dense(1, activation='relu')(merged_h1)\n",
    "\t\tmodel  = Model(input=[state_input,action_input], output=output)\n",
    "\t\t\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, action_input, model\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                               Model Training                              #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef remember(self, cur_state, action, reward, new_state, done):\n",
    "\t\tself.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "\tdef _train_actor(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, _ = sample\n",
    "\t\t\tpredicted_action = self.actor_model.predict(cur_state)\n",
    "\t\t\tgrads = self.sess.run(self.critic_grads, feed_dict={\n",
    "\t\t\t\tself.critic_state_input:  cur_state,\n",
    "\t\t\t\tself.critic_action_input: predicted_action\n",
    "\t\t\t})[0]\n",
    "\n",
    "\t\t\tself.sess.run(self.optimize, feed_dict={\n",
    "\t\t\t\tself.actor_state_input: cur_state,\n",
    "\t\t\t\tself.actor_critic_grad: grads\n",
    "\t\t\t})\n",
    "            \n",
    "\tdef _train_critic(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, done = sample\n",
    "\t\t\tif not done:\n",
    "\t\t\t\ttarget_action = self.target_actor_model.predict(new_state)\n",
    "\t\t\t\tfuture_reward = self.target_critic_model.predict(\n",
    "\t\t\t\t\t[new_state, target_action])[0][0]\n",
    "\t\t\t\treward += self.gamma * future_reward\n",
    "\t\t\tself.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\t\t\n",
    "\tdef train(self):\n",
    "\t\tbatch_size = 32\n",
    "\t\tif len(self.memory) < batch_size:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\trewards = []\n",
    "\t\tsamples = random.sample(self.memory, batch_size)\n",
    "\t\tself._train_critic(samples)\n",
    "\t\tself._train_actor(samples)\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                         Target Model Updating                             #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef _update_actor_target(self):\n",
    "\t\tactor_model_weights  = self.actor_model.get_weights()\n",
    "\t\tactor_target_weights = self.target_critic_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(actor_target_weights)):\n",
    "\t\t\tactor_target_weights[i] = actor_model_weights[i]\n",
    "\t\tself.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "\tdef _update_critic_target(self):\n",
    "\t\tcritic_model_weights  = self.critic_model.get_weights()\n",
    "\t\tcritic_target_weights = self.critic_target_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(critic_target_weights)):\n",
    "\t\t\tcritic_target_weights[i] = critic_model_weights[i]\n",
    "\t\tself.critic_target_model.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "\tdef update_target(self):\n",
    "\t\tself._update_actor_target()\n",
    "\t\tself._update_critic_target()\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Predictions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef act(self, cur_state):\n",
    "\t\tself.epsilon *= self.epsilon_decay\n",
    "\t\tif np.random.random() < self.epsilon:\n",
    "\t\t\treturn self.env.action_space.sample()\n",
    "\t\treturn self.actor_model.predict(cur_state)\n",
    "\n",
    "def main():\n",
    "\tsess = tf.Session()\n",
    "\tK.set_session(sess)\n",
    "\tenv = gym.make(\"Pendulum-v0\")\n",
    "\tactor_critic = ActorCritic(env, sess)\n",
    "\n",
    "\tnum_trials = 10000\n",
    "\ttrial_len  = 500\n",
    "\n",
    "\tcur_state = env.reset()\n",
    "\taction = env.action_space.sample()\n",
    "\twhile True:\n",
    "\t\tenv.render()\n",
    "\t\tcur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "\t\taction = actor_critic.act(cur_state)\n",
    "\t\taction = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "\t\tactor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "\t\tactor_critic.train()\n",
    "\n",
    "\t\tcur_state = new_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN - Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Failed to complete in trial 0\n",
      "Failed to complete in trial 1\n",
      "Failed to complete in trial 2\n",
      "Failed to complete in trial 3\n",
      "Failed to complete in trial 4\n",
      "Failed to complete in trial 5\n",
      "Failed to complete in trial 6\n",
      "Failed to complete in trial 7\n",
      "Failed to complete in trial 8\n",
      "Failed to complete in trial 9\n",
      "Failed to complete in trial 10\n",
      "Failed to complete in trial 11\n",
      "Failed to complete in trial 12\n",
      "Failed to complete in trial 13\n",
      "Failed to complete in trial 14\n",
      "Failed to complete in trial 15\n",
      "Failed to complete in trial 16\n",
      "Failed to complete in trial 17\n",
      "Failed to complete in trial 18\n",
      "Failed to complete in trial 19\n",
      "Failed to complete in trial 20\n",
      "Failed to complete in trial 21\n",
      "Failed to complete in trial 22\n",
      "Failed to complete in trial 23\n",
      "Failed to complete in trial 24\n",
      "Failed to complete in trial 25\n",
      "Failed to complete in trial 26\n",
      "Failed to complete in trial 27\n",
      "Failed to complete in trial 28\n",
      "Failed to complete in trial 29\n",
      "Failed to complete in trial 30\n",
      "Failed to complete in trial 31\n",
      "Failed to complete in trial 32\n",
      "Failed to complete in trial 33\n",
      "Failed to complete in trial 34\n",
      "Failed to complete in trial 35\n",
      "Failed to complete in trial 36\n",
      "Failed to complete in trial 37\n",
      "Failed to complete in trial 38\n",
      "Failed to complete in trial 39\n",
      "Failed to complete in trial 40\n",
      "Failed to complete in trial 41\n",
      "Failed to complete in trial 42\n",
      "Failed to complete in trial 43\n",
      "Failed to complete in trial 44\n",
      "Failed to complete in trial 45\n",
      "Failed to complete in trial 46\n",
      "Failed to complete in trial 47\n",
      "Failed to complete in trial 48\n",
      "Failed to complete in trial 49\n",
      "Failed to complete in trial 50\n",
      "Failed to complete in trial 51\n",
      "Failed to complete in trial 52\n",
      "Failed to complete in trial 53\n",
      "Failed to complete in trial 54\n",
      "Failed to complete in trial 55\n",
      "Failed to complete in trial 56\n",
      "Failed to complete in trial 57\n",
      "Failed to complete in trial 58\n",
      "Failed to complete in trial 59\n",
      "Failed to complete in trial 60\n",
      "Failed to complete in trial 61\n",
      "Failed to complete in trial 62\n",
      "Failed to complete in trial 63\n",
      "Failed to complete in trial 64\n",
      "Failed to complete in trial 65\n",
      "Failed to complete in trial 66\n",
      "Failed to complete in trial 67\n",
      "Failed to complete in trial 68\n",
      "Failed to complete in trial 69\n",
      "Failed to complete in trial 70\n",
      "Failed to complete in trial 71\n",
      "Failed to complete in trial 72\n",
      "Failed to complete in trial 73\n",
      "Failed to complete in trial 74\n",
      "Failed to complete in trial 75\n",
      "Failed to complete in trial 76\n",
      "Failed to complete in trial 77\n",
      "Failed to complete in trial 78\n",
      "Failed to complete in trial 79\n",
      "Failed to complete in trial 80\n",
      "Failed to complete in trial 81\n",
      "Failed to complete in trial 82\n",
      "Failed to complete in trial 83\n",
      "Failed to complete in trial 84\n",
      "Failed to complete in trial 85\n",
      "Failed to complete in trial 86\n",
      "Failed to complete in trial 87\n",
      "Failed to complete in trial 88\n",
      "Failed to complete in trial 89\n",
      "Failed to complete in trial 90\n",
      "Failed to complete in trial 91\n",
      "Failed to complete in trial 92\n",
      "Failed to complete in trial 93\n",
      "Failed to complete in trial 94\n",
      "Failed to complete in trial 95\n",
      "Failed to complete in trial 96\n",
      "Failed to complete in trial 97\n",
      "Failed to complete in trial 98\n",
      "Failed to complete in trial 99\n",
      "Failed to complete in trial 100\n",
      "Failed to complete in trial 101\n",
      "Failed to complete in trial 102\n",
      "Failed to complete in trial 103\n",
      "Failed to complete in trial 104\n",
      "Failed to complete in trial 105\n",
      "Failed to complete in trial 106\n",
      "Failed to complete in trial 107\n",
      "Failed to complete in trial 108\n",
      "Failed to complete in trial 109\n",
      "Failed to complete in trial 110\n",
      "Failed to complete in trial 111\n",
      "Failed to complete in trial 112\n",
      "Failed to complete in trial 113\n",
      "Failed to complete in trial 114\n",
      "Failed to complete in trial 115\n",
      "Failed to complete in trial 116\n",
      "Failed to complete in trial 117\n",
      "Failed to complete in trial 118\n",
      "Failed to complete in trial 119\n",
      "Failed to complete in trial 120\n",
      "Failed to complete in trial 121\n",
      "Failed to complete in trial 122\n",
      "Failed to complete in trial 123\n",
      "Failed to complete in trial 124\n",
      "Failed to complete in trial 125\n",
      "Failed to complete in trial 126\n",
      "Failed to complete in trial 127\n",
      "Failed to complete in trial 128\n",
      "Failed to complete in trial 129\n",
      "Failed to complete in trial 130\n",
      "Failed to complete in trial 131\n",
      "Failed to complete in trial 132\n",
      "Failed to complete in trial 133\n",
      "Failed to complete in trial 134\n",
      "Failed to complete in trial 135\n",
      "Failed to complete in trial 136\n",
      "Failed to complete in trial 137\n",
      "Failed to complete in trial 138\n",
      "Failed to complete in trial 139\n",
      "Failed to complete in trial 140\n",
      "Failed to complete in trial 141\n",
      "Failed to complete in trial 142\n",
      "Failed to complete in trial 143\n",
      "Failed to complete in trial 144\n",
      "Failed to complete in trial 145\n",
      "Failed to complete in trial 146\n",
      "Failed to complete in trial 147\n",
      "Failed to complete in trial 148\n",
      "Failed to complete in trial 149\n",
      "Failed to complete in trial 150\n",
      "Failed to complete in trial 151\n",
      "Failed to complete in trial 152\n",
      "Failed to complete in trial 153\n",
      "Failed to complete in trial 154\n",
      "Failed to complete in trial 155\n",
      "Failed to complete in trial 156\n",
      "Failed to complete in trial 157\n",
      "Failed to complete in trial 158\n",
      "Failed to complete in trial 159\n",
      "Failed to complete in trial 160\n",
      "Failed to complete in trial 161\n",
      "Failed to complete in trial 162\n",
      "Failed to complete in trial 163\n",
      "Failed to complete in trial 164\n",
      "Failed to complete in trial 165\n",
      "Failed to complete in trial 166\n",
      "Failed to complete in trial 167\n",
      "Failed to complete in trial 168\n",
      "Failed to complete in trial 169\n",
      "Failed to complete in trial 170\n",
      "Failed to complete in trial 171\n",
      "Failed to complete in trial 172\n",
      "Failed to complete in trial 173\n",
      "Failed to complete in trial 174\n",
      "Failed to complete in trial 175\n",
      "Failed to complete in trial 176\n",
      "Failed to complete in trial 177\n",
      "Failed to complete in trial 178\n",
      "Failed to complete in trial 179\n",
      "Failed to complete in trial 180\n",
      "Failed to complete in trial 181\n",
      "Failed to complete in trial 182\n",
      "Failed to complete in trial 183\n",
      "Failed to complete in trial 184\n",
      "Failed to complete in trial 185\n",
      "Failed to complete in trial 186\n",
      "Failed to complete in trial 187\n",
      "Failed to complete in trial 188\n",
      "Failed to complete in trial 189\n",
      "Failed to complete in trial 190\n",
      "Failed to complete in trial 191\n",
      "Failed to complete in trial 192\n",
      "Failed to complete in trial 193\n",
      "Failed to complete in trial 194\n",
      "Failed to complete in trial 195\n",
      "Failed to complete in trial 196\n",
      "Failed to complete in trial 197\n",
      "Failed to complete in trial 198\n",
      "Failed to complete in trial 199\n",
      "Failed to complete in trial 200\n",
      "Failed to complete in trial 201\n",
      "Failed to complete in trial 202\n",
      "Failed to complete in trial 203\n",
      "Failed to complete in trial 204\n",
      "Failed to complete in trial 205\n",
      "Failed to complete in trial 206\n",
      "Failed to complete in trial 207\n",
      "Failed to complete in trial 208\n",
      "Failed to complete in trial 209\n",
      "Failed to complete in trial 210\n",
      "Failed to complete in trial 211\n",
      "Failed to complete in trial 212\n",
      "Failed to complete in trial 213\n",
      "Failed to complete in trial 214\n",
      "Failed to complete in trial 215\n",
      "Failed to complete in trial 216\n",
      "Failed to complete in trial 217\n",
      "Failed to complete in trial 218\n",
      "Failed to complete in trial 219\n",
      "Failed to complete in trial 220\n",
      "Failed to complete in trial 221\n",
      "Failed to complete in trial 222\n",
      "Failed to complete in trial 223\n",
      "Failed to complete in trial 224\n",
      "Failed to complete in trial 225\n",
      "Failed to complete in trial 226\n",
      "Failed to complete in trial 227\n",
      "Failed to complete in trial 228\n",
      "Failed to complete in trial 229\n",
      "Failed to complete in trial 230\n",
      "Failed to complete in trial 231\n",
      "Failed to complete in trial 232\n",
      "Failed to complete in trial 233\n",
      "Failed to complete in trial 234\n",
      "Failed to complete in trial 235\n",
      "Failed to complete in trial 236\n",
      "Failed to complete in trial 237\n",
      "Failed to complete in trial 238\n",
      "Failed to complete in trial 239\n",
      "Failed to complete in trial 240\n",
      "Failed to complete in trial 241\n",
      "Failed to complete in trial 242\n",
      "Failed to complete in trial 243\n",
      "Failed to complete in trial 244\n",
      "Failed to complete in trial 245\n",
      "Failed to complete in trial 246\n",
      "Failed to complete in trial 247\n",
      "Failed to complete in trial 248\n",
      "Failed to complete in trial 249\n",
      "Failed to complete in trial 250\n",
      "Failed to complete in trial 251\n",
      "Failed to complete in trial 252\n",
      "Failed to complete in trial 253\n",
      "Failed to complete in trial 254\n",
      "Failed to complete in trial 255\n",
      "Failed to complete in trial 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to complete in trial 257\n",
      "Failed to complete in trial 258\n",
      "Failed to complete in trial 259\n",
      "Failed to complete in trial 260\n",
      "Failed to complete in trial 261\n",
      "Failed to complete in trial 262\n",
      "Failed to complete in trial 263\n",
      "Completed in 264 trials\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.005\n",
    "        self.tau = .125\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "def main():\n",
    "    env     = gym.make(\"MountainCar-v0\")\n",
    "    gamma   = 0.9\n",
    "    epsilon = .95\n",
    "\n",
    "    trials  = 1000\n",
    "    trial_len = 500\n",
    "\n",
    "    # updateTargetNetwork = 1000\n",
    "    dqn_agent = DQN(env=env)\n",
    "    steps = []\n",
    "    for trial in range(trials):\n",
    "        cur_state = env.reset().reshape(1,2)\n",
    "        for step in range(trial_len):\n",
    "            action = dqn_agent.act(cur_state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            # reward = reward if not done else -20\n",
    "            new_state = new_state.reshape(1,2)\n",
    "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "            dqn_agent.target_train() # iterates target model\n",
    "\n",
    "            cur_state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        if step >= 199:\n",
    "            print(\"Failed to complete in trial {}\".format(trial))\n",
    "            if step % 10 == 0:\n",
    "                dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "        else:\n",
    "            print(\"Completed in {} trials\".format(trial))\n",
    "            dqn_agent.save_model(\"success.model\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3c7430f000c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m199\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to complete in trial {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trial-{}.model\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trial' is not defined"
     ]
    }
   ],
   "source": [
    "env     = gym.make(\"MountainCar-v0\")\n",
    "trial_len = 500\n",
    "cur_state = env.reset().reshape(1,2)\n",
    "dqn_agent = DQN(env=env)\n",
    "steps = []\n",
    "for step in range(trial_len):\n",
    "    action = dqn_agent.act(cur_state)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # reward = reward if not done else -20\n",
    "    new_state = new_state.reshape(1,2)\n",
    "    dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "    dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "    dqn_agent.target_train() # iterates target model\n",
    "\n",
    "    cur_state = new_state\n",
    "if step >= 199:\n",
    "    print(\"Failed to complete in trial {}\".format(trial))\n",
    "    if step % 10 == 0:\n",
    "        dqn_agent.save_model(\"trial-{}.model\".format(trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
